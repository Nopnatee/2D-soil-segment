{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fdd071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,927,108\n",
      "Input shape: torch.Size([2, 3, 256, 256])\n",
      "Output shape: torch.Size([2, 4, 256, 256])\n",
      "Predictions shape: torch.Size([2, 256, 256])\n",
      "Training loss: 1.4427\n",
      "\n",
      "Simplified U-Net Features:\n",
      "- 3 encoder/decoder levels (vs 4 in full U-Net)\n",
      "- Fewer channels: 32→64→128→256 (vs 64→128→256→512→1024)\n",
      "- Instance normalization for small datasets\n",
      "- Simple skip connections\n",
      "- Dropout for regularization\n",
      "- ~10x fewer parameters than full U-Net\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Simple double convolution block with normalization and activation\"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_ch, affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Simplified U-Net for bead classification\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = ConvBlock(3, 32)\n",
    "        self.enc2 = ConvBlock(32, 64)\n",
    "        self.enc3 = ConvBlock(64, 128)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(128, 256)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(256, 128)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(128, 64)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(64, 32)\n",
    "        \n",
    "        # Final layer\n",
    "        self.final = nn.Conv2d(32, n_classes, 1)\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        b = self.dropout(b)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d3 = self.up3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Final output\n",
    "        out = self.final(d1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create simplified model\n",
    "    model = SimpleUNet(n_classes=4)  # 10 bead classes\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Test with sample input\n",
    "    x = torch.randn(2, 3, 256, 256)  # batch_size=2, channels=3, height=256, width=256\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Training example\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    # Dummy training step\n",
    "    target = torch.randint(0, 4, (2, 256, 256))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Training loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\nSimplified U-Net Features:\")\n",
    "    print(\"- 3 encoder/decoder levels (vs 4 in full U-Net)\")\n",
    "    print(\"- Fewer channels: 32→64→128→256 (vs 64→128→256→512→1024)\")\n",
    "    print(\"- Instance normalization for small datasets\")\n",
    "    print(\"- Simple skip connections\")\n",
    "    print(\"- Dropout for regularization\")\n",
    "    print(\"- ~10x fewer parameters than full U-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fed83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/train\\\\images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 419\u001b[0m\n\u001b[0;32m    415\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mevaluate_model(test_loader)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 391\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    388\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleUNet(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Create data loaders (adjust paths as needed)\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[0;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[0;32m    400\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SegmentationTrainer(\n\u001b[0;32m    401\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    402\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m    406\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 352\u001b[0m, in \u001b[0;36mcreate_data_loaders\u001b[1;34m(train_dir, val_dir, test_dir, batch_size, img_size)\u001b[0m\n\u001b[0;32m    345\u001b[0m target_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    346\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((img_size, img_size), interpolation\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mInterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST),\n\u001b[0;32m    347\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m    348\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m    349\u001b[0m ])\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBeadDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmasks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m BeadDataset(\n\u001b[0;32m    360\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(val_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    361\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(val_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    362\u001b[0m     transform\u001b[38;5;241m=\u001b[39mval_transform,\n\u001b[0;32m    363\u001b[0m     target_transform\u001b[38;5;241m=\u001b[39mtarget_transform\n\u001b[0;32m    364\u001b[0m )\n\u001b[0;32m    366\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m BeadDataset(\n\u001b[0;32m    367\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    368\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    369\u001b[0m     transform\u001b[38;5;241m=\u001b[39mval_transform,\n\u001b[0;32m    370\u001b[0m     target_transform\u001b[38;5;241m=\u001b[39mtarget_transform\n\u001b[0;32m    371\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mBeadDataset.__init__\u001b[1;34m(self, image_dir, mask_dir, transform, target_transform)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;241m=\u001b[39m target_transform\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Get all image files\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/train\\\\images'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class BeadDataset(Dataset):\n",
    "    \"\"\"Custom dataset for bead segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Get all image files\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png'))\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Grayscale mask\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class DiceScore(nn.Module):\n",
    "    \"\"\"Dice coefficient for segmentation evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        dice_scores = []\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            pred_i = pred[:, i]\n",
    "            target_i = (target == i).float()\n",
    "            \n",
    "            intersection = (pred_i * target_i).sum()\n",
    "            union = pred_i.sum() + target_i.sum()\n",
    "            \n",
    "            dice = (2 * intersection + self.smooth) / (union + self.smooth)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        return torch.stack(dice_scores).mean()\n",
    "\n",
    "\n",
    "class SegmentationTrainer:\n",
    "    \"\"\"Complete training pipeline for SimpleUNet\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, train_loader, val_loader, n_classes=4):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Loss functions\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.dice_metric = DiceScore(n_classes)\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_dice': [],\n",
    "            'val_dice': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        \n",
    "        # Best model tracking\n",
    "        self.best_dice = 0.0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        # Set model to training mode\n",
    "        self.model.train() \n",
    "        running_loss = 0.0\n",
    "        running_dice = 0.0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.criterion(output, target)\n",
    "            dice = self.dice_metric(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Dice': f'{dice.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_dice = running_dice / len(self.train_loader)\n",
    "        \n",
    "        return avg_loss, avg_dice\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_dice = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation')\n",
    "            for data, target in pbar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                dice = self.dice_metric(output, target)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_dice += dice.item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Dice': f'{dice.item():.4f}'\n",
    "                })\n",
    "        \n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_dice = running_dice / len(self.val_loader)\n",
    "        \n",
    "        return avg_loss, avg_dice\n",
    "    \n",
    "    def train(self, num_epochs, save_dir='checkpoints'):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_dice = self.train_epoch()\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_dice = self.validate()\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Save metrics\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_dice'].append(train_dice)\n",
    "            self.history['val_dice'].append(val_dice)\n",
    "            self.history['lr'].append(current_lr)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_dice > self.best_dice:\n",
    "                self.best_dice = val_dice\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'best_dice': self.best_dice,\n",
    "                    'history': self.history\n",
    "                }, os.path.join(save_dir, 'best_model.pth'))\n",
    "                print(f\"New best model saved! Dice: {val_dice:.4f}\")\n",
    "            \n",
    "            # Regular checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'history': self.history\n",
    "                }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "            print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
    "        print(f\"Best validation Dice: {self.best_dice:.4f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training curves\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train Loss')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Val Loss')\n",
    "        axes[0, 0].set_title('Loss Curves')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Dice curves\n",
    "        axes[0, 1].plot(self.history['train_dice'], label='Train Dice')\n",
    "        axes[0, 1].plot(self.history['val_dice'], label='Val Dice')\n",
    "        axes[0, 1].set_title('Dice Score Curves')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Dice Score')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Learning rate\n",
    "        axes[1, 0].plot(self.history['lr'])\n",
    "        axes[1, 0].set_title('Learning Rate')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Combined metrics\n",
    "        axes[1, 1].plot(self.history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "        ax2 = axes[1, 1].twinx()\n",
    "        ax2.plot(self.history['val_dice'], label='Val Dice', color='orange', alpha=0.7)\n",
    "        axes[1, 1].set_title('Validation Metrics')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss', color='blue')\n",
    "        ax2.set_ylabel('Dice Score', color='orange')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate_model(self, test_loader):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(test_loader, desc='Evaluating'):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                output = self.model(data)\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                \n",
    "                all_predictions.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_targets, all_predictions))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(all_targets, all_predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        \n",
    "        return all_predictions, all_targets\n",
    "\n",
    "\n",
    "def create_data_loaders(data_dir, batch_size=8, img_size=256, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create data loaders with automatic train/val/test split from a single directory\n",
    "    \n",
    "    The directory structure:\n",
    "    UNET_dataset/\n",
    "    ├── images/\n",
    "    │   ├── img1.jpg\n",
    "    │   ├── img2.jpg\n",
    "    │   └── ...\n",
    "    └── masks/\n",
    "        ├── img1.png\n",
    "        ├── img2.png\n",
    "        └── ...\n",
    "\n",
    "    Args:\n",
    "        data_dir: Single directory containing 'images' and 'masks' subdirectories\n",
    "        batch_size: Batch size for data loaders\n",
    "        img_size: Image size for resizing\n",
    "        train_ratio: Proportion of data for training (default: 0.7)\n",
    "        val_ratio: Proportion of data for validation (default: 0.2)\n",
    "        test_ratio: Proportion of data for testing (default: 0.1)\n",
    "        random_seed: Random seed for reproducible splits\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate ratios\n",
    "    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:\n",
    "        raise ValueError(\"train_ratio + val_ratio + test_ratio must equal 1.0\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.squeeze().long())\n",
    "    ])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_dataset = BeadDataset(\n",
    "        os.path.join(data_dir, 'images'),\n",
    "        os.path.join(data_dir, 'masks'),\n",
    "        transform=None,  # We'll apply transforms later\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    # Get total number of samples\n",
    "    total_samples = len(full_dataset)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    train_size = int(train_ratio * total_samples)\n",
    "    val_size = int(val_ratio * total_samples)\n",
    "    test_size = total_samples - train_size - val_size  # Remaining samples\n",
    "    \n",
    "    # Create indices for splitting\n",
    "    indices = list(range(total_samples))\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices, temp_indices = train_test_split(\n",
    "        indices, \n",
    "        train_size=train_size, \n",
    "        random_state=random_seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Split remaining indices into val and test\n",
    "    val_indices, test_indices = train_test_split(\n",
    "        temp_indices,\n",
    "        train_size=val_size,\n",
    "        random_state=random_seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Create datasets with different transforms\n",
    "    train_dataset = BeadDataset(\n",
    "        os.path.join(data_dir, 'images'),\n",
    "        os.path.join(data_dir, 'masks'),\n",
    "        transform=train_transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = BeadDataset(\n",
    "        os.path.join(data_dir, 'images'),\n",
    "        os.path.join(data_dir, 'masks'),\n",
    "        transform=val_test_transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = BeadDataset(\n",
    "        os.path.join(data_dir, 'images'),\n",
    "        os.path.join(data_dir, 'masks'),\n",
    "        transform=val_test_transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "    \n",
    "    # Create subsets using indices\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(val_dataset, val_indices)\n",
    "    test_subset = Subset(test_dataset, test_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"  Total samples: {total_samples}\")\n",
    "    print(f\"  Train: {len(train_indices)} samples ({len(train_indices)/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Validation: {len(val_indices)} samples ({len(val_indices)/total_samples*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_indices)} samples ({len(test_indices)/total_samples*100:.1f}%)\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training script\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleUNet(n_classes=4)\n",
    "    \n",
    "    # Create data loaders (adjust paths as needed)\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    data_dir='UNET_dataset',\n",
    "    batch_size=8,\n",
    "    img_size=256\n",
    ")\n",
    "    # Create trainer\n",
    "    trainer = SegmentationTrainer(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        n_classes=4\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train(num_epochs=50)\n",
    "    \n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    trainer.evaluate_model(test_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
